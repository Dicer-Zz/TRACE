{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zcwang/.conda/envs/trace/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/zcwang/.conda/envs/trace/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraModel, LoraConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /home/zcwang/.conda/envs/trace/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /home/zcwang/.conda/envs/trace/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
     ]
    }
   ],
   "source": [
    "model_name_or_path = \"/home/zcwang/data/model/google/gemma-2b-it\"\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=8,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "\n",
    "lora_model = LoraModel(model, config, adapter_name=\"task_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraModel(\n",
       "  (model): GemmaForCausalLM(\n",
       "    (model): GemmaModel(\n",
       "      (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "      (layers): ModuleList(\n",
       "        (0-17): 18 x GemmaDecoderLayer(\n",
       "          (self_attn): GemmaSdpaAttention(\n",
       "            (q_proj): lora.Linear(\n",
       "              (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (task_0): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (task_0): Linear(in_features=2048, out_features=8, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (task_0): Linear(in_features=8, out_features=2048, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "            )\n",
       "            (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "            (v_proj): lora.Linear(\n",
       "              (base_layer): Linear(in_features=2048, out_features=256, bias=False)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (task_0): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (task_0): Linear(in_features=2048, out_features=8, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (task_0): Linear(in_features=8, out_features=256, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "            )\n",
       "            (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (rotary_emb): GemmaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): GemmaMLP(\n",
       "            (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "            (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "            (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
       "            (act_fn): GELUActivation()\n",
       "          )\n",
       "          (input_layernorm): GemmaRMSNorm()\n",
       "          (post_attention_layernorm): GemmaRMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (norm): GemmaRMSNorm()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaModel(\n",
       "  (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "  (layers): ModuleList(\n",
       "    (0-17): 18 x GemmaDecoderLayer(\n",
       "      (self_attn): GemmaSdpaAttention(\n",
       "        (q_proj): lora.Linear(\n",
       "          (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (lora_dropout): ModuleDict(\n",
       "            (task_0): Identity()\n",
       "          )\n",
       "          (lora_A): ModuleDict(\n",
       "            (task_0): Linear(in_features=2048, out_features=8, bias=False)\n",
       "          )\n",
       "          (lora_B): ModuleDict(\n",
       "            (task_0): Linear(in_features=8, out_features=2048, bias=False)\n",
       "          )\n",
       "          (lora_embedding_A): ParameterDict()\n",
       "          (lora_embedding_B): ParameterDict()\n",
       "        )\n",
       "        (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "        (v_proj): lora.Linear(\n",
       "          (base_layer): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (lora_dropout): ModuleDict(\n",
       "            (task_0): Identity()\n",
       "          )\n",
       "          (lora_A): ModuleDict(\n",
       "            (task_0): Linear(in_features=2048, out_features=8, bias=False)\n",
       "          )\n",
       "          (lora_B): ModuleDict(\n",
       "            (task_0): Linear(in_features=8, out_features=256, bias=False)\n",
       "          )\n",
       "          (lora_embedding_A): ParameterDict()\n",
       "          (lora_embedding_B): ParameterDict()\n",
       "        )\n",
       "        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        (rotary_emb): GemmaRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): GemmaMLP(\n",
       "        (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "        (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "        (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
       "        (act_fn): GELUActivation()\n",
       "      )\n",
       "      (input_layernorm): GemmaRMSNorm()\n",
       "      (post_attention_layernorm): GemmaRMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (norm): GemmaRMSNorm()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_model.base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model.save_pretrained(\"/home/zcwang/data/model/google/gemma-2b-it-lora\") # save the whole model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training': True,\n",
       " '_parameters': OrderedDict(),\n",
       " '_buffers': OrderedDict(),\n",
       " '_non_persistent_buffers_set': set(),\n",
       " '_backward_pre_hooks': OrderedDict(),\n",
       " '_backward_hooks': OrderedDict(),\n",
       " '_is_full_backward_hook': None,\n",
       " '_forward_hooks': OrderedDict(),\n",
       " '_forward_hooks_with_kwargs': OrderedDict(),\n",
       " '_forward_hooks_always_called': OrderedDict(),\n",
       " '_forward_pre_hooks': OrderedDict(),\n",
       " '_forward_pre_hooks_with_kwargs': OrderedDict(),\n",
       " '_state_dict_hooks': OrderedDict(),\n",
       " '_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_post_hooks': OrderedDict(),\n",
       " '_modules': OrderedDict([('model',\n",
       "               GemmaForCausalLM(\n",
       "                 (model): GemmaModel(\n",
       "                   (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "                   (layers): ModuleList(\n",
       "                     (0-17): 18 x GemmaDecoderLayer(\n",
       "                       (self_attn): GemmaSdpaAttention(\n",
       "                         (q_proj): lora.Linear(\n",
       "                           (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                           (lora_dropout): ModuleDict(\n",
       "                             (task_0): Identity()\n",
       "                           )\n",
       "                           (lora_A): ModuleDict(\n",
       "                             (task_0): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                           )\n",
       "                           (lora_B): ModuleDict(\n",
       "                             (task_0): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                           )\n",
       "                           (lora_embedding_A): ParameterDict()\n",
       "                           (lora_embedding_B): ParameterDict()\n",
       "                         )\n",
       "                         (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "                         (v_proj): lora.Linear(\n",
       "                           (base_layer): Linear(in_features=2048, out_features=256, bias=False)\n",
       "                           (lora_dropout): ModuleDict(\n",
       "                             (task_0): Identity()\n",
       "                           )\n",
       "                           (lora_A): ModuleDict(\n",
       "                             (task_0): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                           )\n",
       "                           (lora_B): ModuleDict(\n",
       "                             (task_0): Linear(in_features=8, out_features=256, bias=False)\n",
       "                           )\n",
       "                           (lora_embedding_A): ParameterDict()\n",
       "                           (lora_embedding_B): ParameterDict()\n",
       "                         )\n",
       "                         (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                         (rotary_emb): GemmaRotaryEmbedding()\n",
       "                       )\n",
       "                       (mlp): GemmaMLP(\n",
       "                         (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "                         (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "                         (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
       "                         (act_fn): GELUActivation()\n",
       "                       )\n",
       "                       (input_layernorm): GemmaRMSNorm()\n",
       "                       (post_attention_layernorm): GemmaRMSNorm()\n",
       "                     )\n",
       "                   )\n",
       "                   (norm): GemmaRMSNorm()\n",
       "                 )\n",
       "                 (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       "               ))]),\n",
       " 'targeted_module_names': ['model.layers.0.self_attn.q_proj',\n",
       "  'model.layers.0.self_attn.v_proj',\n",
       "  'model.layers.1.self_attn.q_proj',\n",
       "  'model.layers.1.self_attn.v_proj',\n",
       "  'model.layers.2.self_attn.q_proj',\n",
       "  'model.layers.2.self_attn.v_proj',\n",
       "  'model.layers.3.self_attn.q_proj',\n",
       "  'model.layers.3.self_attn.v_proj',\n",
       "  'model.layers.4.self_attn.q_proj',\n",
       "  'model.layers.4.self_attn.v_proj',\n",
       "  'model.layers.5.self_attn.q_proj',\n",
       "  'model.layers.5.self_attn.v_proj',\n",
       "  'model.layers.6.self_attn.q_proj',\n",
       "  'model.layers.6.self_attn.v_proj',\n",
       "  'model.layers.7.self_attn.q_proj',\n",
       "  'model.layers.7.self_attn.v_proj',\n",
       "  'model.layers.8.self_attn.q_proj',\n",
       "  'model.layers.8.self_attn.v_proj',\n",
       "  'model.layers.9.self_attn.q_proj',\n",
       "  'model.layers.9.self_attn.v_proj',\n",
       "  'model.layers.10.self_attn.q_proj',\n",
       "  'model.layers.10.self_attn.v_proj',\n",
       "  'model.layers.11.self_attn.q_proj',\n",
       "  'model.layers.11.self_attn.v_proj',\n",
       "  'model.layers.12.self_attn.q_proj',\n",
       "  'model.layers.12.self_attn.v_proj',\n",
       "  'model.layers.13.self_attn.q_proj',\n",
       "  'model.layers.13.self_attn.v_proj',\n",
       "  'model.layers.14.self_attn.q_proj',\n",
       "  'model.layers.14.self_attn.v_proj',\n",
       "  'model.layers.15.self_attn.q_proj',\n",
       "  'model.layers.15.self_attn.v_proj',\n",
       "  'model.layers.16.self_attn.q_proj',\n",
       "  'model.layers.16.self_attn.v_proj',\n",
       "  'model.layers.17.self_attn.q_proj',\n",
       "  'model.layers.17.self_attn.v_proj'],\n",
       " 'peft_config': {'task_0': LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type=None, inference_mode=False, r=8, target_modules={'v_proj', 'q_proj'}, lora_alpha=8, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False)},\n",
       " 'active_adapter': 'task_0'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_model.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/zcwang/TRACE/playground/lora_test.ipynb 单元格 6\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B58/home/zcwang/TRACE/playground/lora_test.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpeft\u001b[39;00m \u001b[39mimport\u001b[39;00m PeftModel\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B58/home/zcwang/TRACE/playground/lora_test.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(lora_model, PeftModel)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "assert isinstance(lora_model, PeftModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Adapter t does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/zcwang/TRACE/playground/lora_test.ipynb 单元格 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B75/home/zcwang/TRACE/playground/lora_test.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m lora_model\u001b[39m.\u001b[39;49madd_weighted_adapter(\u001b[39m\"\u001b[39;49m\u001b[39mtask_0\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m0.5\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mtask_1\u001b[39;49m\u001b[39m\"\u001b[39;49m, combination_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcat\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/.conda/envs/trace/lib/python3.10/site-packages/peft/tuners/lora/model.py:438\u001b[0m, in \u001b[0;36mLoraModel.add_weighted_adapter\u001b[0;34m(self, adapters, weights, adapter_name, combination_type, svd_rank, svd_clamp, svd_full_matrices, svd_driver, density, majority_sign_method)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[39mfor\u001b[39;00m adapter \u001b[39min\u001b[39;00m adapters:\n\u001b[1;32m    437\u001b[0m     \u001b[39mif\u001b[39;00m adapter \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpeft_config\u001b[39m.\u001b[39mkeys()):\n\u001b[0;32m--> 438\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAdapter \u001b[39m\u001b[39m{\u001b[39;00madapter\u001b[39m}\u001b[39;00m\u001b[39m does not exist\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[39m# if there is only one adapter, we can only use linear merging\u001b[39;00m\n\u001b[1;32m    441\u001b[0m combination_type \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlinear\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(adapters) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m combination_type\n",
      "\u001b[0;31mValueError\u001b[0m: Adapter t does not exist"
     ]
    }
   ],
   "source": [
    "lora_model.add_weighted_adapter(\"task_0\", 0.5, \"task_1\", combination_type=\"cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['task_0'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_model.peft_config.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import replace\n",
    "\n",
    "lora_model.model.peft_config[\"task_1\"] = replace(lora_model.model.peft_config[\"task_0\"], lora_alpha=10)\n",
    "lora_model.inject_adapter(lora_model.model, \"task_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model.model.peft_config[\"task_2\"] = replace(lora_model.model.peft_config[\"task_0\"], r=10)\n",
    "lora_model.inject_adapter(lora_model.model, \"task_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraModel(\n",
       "  (model): GemmaForCausalLM(\n",
       "    (model): GemmaModel(\n",
       "      (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "      (layers): ModuleList(\n",
       "        (0-17): 18 x GemmaDecoderLayer(\n",
       "          (self_attn): GemmaSdpaAttention(\n",
       "            (q_proj): lora.Linear(\n",
       "              (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (task_0): Identity()\n",
       "                (task_1): Identity()\n",
       "                (task_2): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (task_0): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                (task_1): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                (task_2): Linear(in_features=2048, out_features=10, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (task_0): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                (task_1): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                (task_2): Linear(in_features=10, out_features=2048, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "            )\n",
       "            (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "            (v_proj): lora.Linear(\n",
       "              (base_layer): Linear(in_features=2048, out_features=256, bias=False)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (task_0): Identity()\n",
       "                (task_1): Identity()\n",
       "                (task_2): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (task_0): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                (task_1): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                (task_2): Linear(in_features=2048, out_features=10, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (task_0): Linear(in_features=8, out_features=256, bias=False)\n",
       "                (task_1): Linear(in_features=8, out_features=256, bias=False)\n",
       "                (task_2): Linear(in_features=10, out_features=256, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "            )\n",
       "            (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (rotary_emb): GemmaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): GemmaMLP(\n",
       "            (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "            (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "            (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
       "            (act_fn): GELUActivation()\n",
       "          )\n",
       "          (input_layernorm): GemmaRMSNorm()\n",
       "          (post_attention_layernorm): GemmaRMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (norm): GemmaRMSNorm()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'task_0'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_model.active_adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['task_0']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_model.active_adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model.active_adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model.set_adapter([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_model.active_adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_model.active_adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraModel(\n",
       "  (model): GemmaForCausalLM(\n",
       "    (model): GemmaModel(\n",
       "      (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "      (layers): ModuleList(\n",
       "        (0-17): 18 x GemmaDecoderLayer(\n",
       "          (self_attn): GemmaSdpaAttention(\n",
       "            (q_proj): lora.Linear(\n",
       "              (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (task_0): Identity()\n",
       "                (task_1): Identity()\n",
       "                (task_2): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (task_0): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                (task_1): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                (task_2): Linear(in_features=2048, out_features=10, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (task_0): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                (task_1): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                (task_2): Linear(in_features=10, out_features=2048, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "            )\n",
       "            (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "            (v_proj): lora.Linear(\n",
       "              (base_layer): Linear(in_features=2048, out_features=256, bias=False)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (task_0): Identity()\n",
       "                (task_1): Identity()\n",
       "                (task_2): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (task_0): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                (task_1): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                (task_2): Linear(in_features=2048, out_features=10, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (task_0): Linear(in_features=8, out_features=256, bias=False)\n",
       "                (task_1): Linear(in_features=8, out_features=256, bias=False)\n",
       "                (task_2): Linear(in_features=10, out_features=256, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "            )\n",
       "            (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (rotary_emb): GemmaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): GemmaMLP(\n",
       "            (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "            (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "            (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
       "            (act_fn): GELUActivation()\n",
       "          )\n",
       "          (input_layernorm): GemmaRMSNorm()\n",
       "          (post_attention_layernorm): GemmaRMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (norm): GemmaRMSNorm()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task_0': LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type=None, inference_mode=False, r=8, target_modules={'v_proj', 'q_proj'}, lora_alpha=8, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False),\n",
       " 'task_1': LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type=None, inference_mode=False, r=8, target_modules={'v_proj', 'q_proj'}, lora_alpha=10, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False),\n",
       " 'task_2': LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type=None, inference_mode=False, r=10, target_modules={'v_proj', 'q_proj'}, lora_alpha=8, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False)}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_model.model.peft_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type=None, inference_mode=False, r=20, target_modules={'v_proj', 'q_proj'}, lora_alpha=8, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replace(lora_model.model.peft_config[\"task_0\"], r=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task_0': LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type=None, inference_mode=False, r=8, target_modules={'v_proj', 'q_proj'}, lora_alpha=8, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False),\n",
       " 'task_1': LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type=None, inference_mode=False, r=8, target_modules={'v_proj', 'q_proj'}, lora_alpha=10, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False),\n",
       " 'task_2': LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type=None, inference_mode=False, r=10, target_modules={'v_proj', 'q_proj'}, lora_alpha=8, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False)}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_model.model.peft_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaConfig {\n",
       "  \"_name_or_path\": \"/home/zcwang/data/model/google/gemma-2b-it\",\n",
       "  \"architectures\": [\n",
       "    \"GemmaForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 2,\n",
       "  \"eos_token_id\": 1,\n",
       "  \"head_dim\": 256,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_size\": 2048,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 16384,\n",
       "  \"max_position_embeddings\": 8192,\n",
       "  \"model_type\": \"gemma\",\n",
       "  \"num_attention_heads\": 8,\n",
       "  \"num_hidden_layers\": 18,\n",
       "  \"num_key_value_heads\": 1,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"rms_norm_eps\": 1e-06,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.38.0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 256000\n",
       "}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
