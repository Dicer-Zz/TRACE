{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zcwang/.conda/envs/trace/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/zcwang/.conda/envs/trace/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:48<00:00, 24.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bos_token <bos> 2\n",
      "eos_token <eos> 1\n",
      "unk_token <unk> 3\n",
      "pad_token <pad> 0\n",
      "additional_special_tokens ['<start_of_turn>', '<end_of_turn>'] [106, 107]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): GemmaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
       "          (act_fn): GELUActivation()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm()\n",
       "        (post_attention_layernorm): GemmaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model_path = \"/home/zcwang/data/model/google/gemma-2b-it\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "# print(tokenizer.special_tokens_map.items())\n",
    "# print special tokens <special_name, cotent, id>\n",
    "for k, v in tokenizer.special_tokens_map.items():\n",
    "    print(k, v, tokenizer.convert_tokens_to_ids(v))\n",
    "\n",
    "model.cuda(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   2,  651, 5929,  603, 4915],\n",
      "        [   0,    0,    2, 2339, 9270]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1],\n",
      "        [0, 0, 1, 1, 1]], device='cuda:0')}\n",
      "torch.Size([6, 50])\n",
      "##############################\n",
      "The dog is happy at being out in nature and enjoys sniffing and chasing after leaves\n",
      "\n",
      "What is the main idea of the passage?\n",
      "\n",
      "The main idea is that the dog is happy and enjoys being out in nature.\n",
      "##############################\n",
      "The dog is happy running alone, but when he is leash-trained, he pulls the rope and runs away.\n",
      "\n",
      "The solution to this problem could be that the leash restricts the dog's ability to move forward.  Sure, the leash\n",
      "##############################\n",
      "The dog is happy.\n",
      "\n",
      "It wagged its tail happily and licked its own face with a satisfied grin.\n",
      "\n",
      "The word \"happy\" was clearly expressed on its face, as its lips curled up in a genuine smile.\n",
      "\n",
      "The dog'\n",
      "##############################\n",
      "So sad, the woman who used to work with me at the bakery was let go. She was a great employee and always willing to help out. She was so friendly and had a smile that could brighten up your day.\n",
      "\n",
      "I\n",
      "##############################\n",
      "So sad, to see how much pain and suffering children put through. To see their innocence torn away and replaced by fear and vulnerability. \n",
      "\n",
      "**And then, the words of hope.**\n",
      "\n",
      "Hope that they will find strength within themselves\n",
      "##############################\n",
      "So sad, I had to delete my old video game collection. üò≠üò≠üò≠\n",
      "\n",
      "**I'm trying to recover the files, but I'm having trouble. Is there any way to recover deleted video game files?**\n",
      "\n",
      "**\n"
     ]
    }
   ],
   "source": [
    "# batched input\n",
    "input_text = [\"The dog is happy\", \"So sad\"]\n",
    "\n",
    "# left padding\n",
    "tokenizer.padding_side = \"left\"\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# tokenize input\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "# move input to cuda\n",
    "input_ids = {k: v.cuda() for k, v in input_ids.items()}\n",
    "print(input_ids)\n",
    "\n",
    "# batched generation and don't do sample but greedy decoding\n",
    "generate_config = {\n",
    "    \"max_length\": 50,\n",
    "    \"num_return_sequences\": 3,\n",
    "    \"do_sample\": True,\n",
    "}\n",
    "batched_output = model.generate(input_ids[\"input_ids\"], **generate_config)\n",
    "print(batched_output.shape)\n",
    "\n",
    "decodings = tokenizer.batch_decode(batched_output, skip_special_tokens=True)\n",
    "for output in decodings:\n",
    "    print('#' * 30)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 50])\n",
      "torch.Size([2, 50])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "argument 'ids': 'list' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/zcwang/TRACE/playground/batched_generation.ipynb ÂçïÂÖÉÊ†º 3\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B58/home/zcwang/TRACE/playground/batched_generation.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     one_by_one_output\u001b[39m.\u001b[39mappend(output)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B58/home/zcwang/TRACE/playground/batched_generation.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39m# print(tokenizer.decode(output[0], skip_special_tokens=True))\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B58/home/zcwang/TRACE/playground/batched_generation.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m decodings \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39;49mbatch_decode(one_by_one_output, skip_special_tokens\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B58/home/zcwang/TRACE/playground/batched_generation.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mfor\u001b[39;00m output \u001b[39min\u001b[39;00m decodings:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B58/home/zcwang/TRACE/playground/batched_generation.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m#\u001b[39m\u001b[39m'\u001b[39m \u001b[39m*\u001b[39m \u001b[39m30\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/trace/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3742\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_decode\u001b[0;34m(self, sequences, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3718\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbatch_decode\u001b[39m(\n\u001b[1;32m   3719\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   3720\u001b[0m     sequences: Union[List[\u001b[39mint\u001b[39m], List[List[\u001b[39mint\u001b[39m]], \u001b[39m\"\u001b[39m\u001b[39mnp.ndarray\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtorch.Tensor\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtf.Tensor\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3723\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   3724\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mstr\u001b[39m]:\n\u001b[1;32m   3725\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3726\u001b[0m \u001b[39m    Convert a list of lists of token ids into a list of strings by calling decode.\u001b[39;00m\n\u001b[1;32m   3727\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3740\u001b[0m \u001b[39m        `List[str]`: The list of decoded sentences.\u001b[39;00m\n\u001b[1;32m   3741\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3742\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m   3743\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecode(\n\u001b[1;32m   3744\u001b[0m             seq,\n\u001b[1;32m   3745\u001b[0m             skip_special_tokens\u001b[39m=\u001b[39mskip_special_tokens,\n\u001b[1;32m   3746\u001b[0m             clean_up_tokenization_spaces\u001b[39m=\u001b[39mclean_up_tokenization_spaces,\n\u001b[1;32m   3747\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   3748\u001b[0m         )\n\u001b[1;32m   3749\u001b[0m         \u001b[39mfor\u001b[39;00m seq \u001b[39min\u001b[39;00m sequences\n\u001b[1;32m   3750\u001b[0m     ]\n",
      "File \u001b[0;32m~/.conda/envs/trace/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3743\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   3718\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbatch_decode\u001b[39m(\n\u001b[1;32m   3719\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   3720\u001b[0m     sequences: Union[List[\u001b[39mint\u001b[39m], List[List[\u001b[39mint\u001b[39m]], \u001b[39m\"\u001b[39m\u001b[39mnp.ndarray\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtorch.Tensor\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtf.Tensor\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3723\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   3724\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mstr\u001b[39m]:\n\u001b[1;32m   3725\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3726\u001b[0m \u001b[39m    Convert a list of lists of token ids into a list of strings by calling decode.\u001b[39;00m\n\u001b[1;32m   3727\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3740\u001b[0m \u001b[39m        `List[str]`: The list of decoded sentences.\u001b[39;00m\n\u001b[1;32m   3741\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m   3742\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m-> 3743\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecode(\n\u001b[1;32m   3744\u001b[0m             seq,\n\u001b[1;32m   3745\u001b[0m             skip_special_tokens\u001b[39m=\u001b[39;49mskip_special_tokens,\n\u001b[1;32m   3746\u001b[0m             clean_up_tokenization_spaces\u001b[39m=\u001b[39;49mclean_up_tokenization_spaces,\n\u001b[1;32m   3747\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   3748\u001b[0m         )\n\u001b[1;32m   3749\u001b[0m         \u001b[39mfor\u001b[39;00m seq \u001b[39min\u001b[39;00m sequences\n\u001b[1;32m   3750\u001b[0m     ]\n",
      "File \u001b[0;32m~/.conda/envs/trace/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3782\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3779\u001b[0m \u001b[39m# Convert inputs to python lists\u001b[39;00m\n\u001b[1;32m   3780\u001b[0m token_ids \u001b[39m=\u001b[39m to_py_obj(token_ids)\n\u001b[0;32m-> 3782\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_decode(\n\u001b[1;32m   3783\u001b[0m     token_ids\u001b[39m=\u001b[39;49mtoken_ids,\n\u001b[1;32m   3784\u001b[0m     skip_special_tokens\u001b[39m=\u001b[39;49mskip_special_tokens,\n\u001b[1;32m   3785\u001b[0m     clean_up_tokenization_spaces\u001b[39m=\u001b[39;49mclean_up_tokenization_spaces,\n\u001b[1;32m   3786\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   3787\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/trace/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:625\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(token_ids, \u001b[39mint\u001b[39m):\n\u001b[1;32m    624\u001b[0m     token_ids \u001b[39m=\u001b[39m [token_ids]\n\u001b[0;32m--> 625\u001b[0m text \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tokenizer\u001b[39m.\u001b[39;49mdecode(token_ids, skip_special_tokens\u001b[39m=\u001b[39;49mskip_special_tokens)\n\u001b[1;32m    627\u001b[0m clean_up_tokenization_spaces \u001b[39m=\u001b[39m (\n\u001b[1;32m    628\u001b[0m     clean_up_tokenization_spaces\n\u001b[1;32m    629\u001b[0m     \u001b[39mif\u001b[39;00m clean_up_tokenization_spaces \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclean_up_tokenization_spaces\n\u001b[1;32m    631\u001b[0m )\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m clean_up_tokenization_spaces:\n",
      "\u001b[0;31mTypeError\u001b[0m: argument 'ids': 'list' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "tokenizer.padding_side = \"right\"\n",
    "generate_config = {\n",
    "    \"max_length\": 50,\n",
    "    \"num_return_sequences\": 2,\n",
    "    \"do_sample\": True,\n",
    "}\n",
    "\n",
    "one_by_one_output = []\n",
    "# generate one by one\n",
    "for i in range(len(input_text)):\n",
    "    input_id = tokenizer(input_text[i], return_tensors=\"pt\", padding=False, truncation=True)\n",
    "    input_id = {k: v.cuda() for k, v in input_id.items()}\n",
    "    output = model.generate(input_id[\"input_ids\"], **generate_config)\n",
    "    print(output.shape)\n",
    "    # one_by_one_output.append(output[0])\n",
    "    one_by_one_output.append(output)\n",
    "    # print(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "\n",
    "\n",
    "decodings = tokenizer.batch_decode(one_by_one_output, skip_special_tokens=True)\n",
    "for output in decodings:\n",
    "    print('#' * 30)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 55])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# [2, 50] cat [5] -> [2, 55]\n",
    "ones = torch.ones(2, 50)\n",
    "zeros = torch.zeros(5)\n",
    "zeros = zeros.repeat(2, 1)\n",
    "print(torch.cat([ones, zeros], dim=1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.mode(\n",
      "values=tensor([2, 4]),\n",
      "indices=tensor([1, 0]))\n",
      "torch.return_types.mode(\n",
      "values=tensor([2, 2, 3]),\n",
      "indices=tensor([0, 0, 0]))\n",
      "torch.return_types.mode(\n",
      "values=tensor([2, 4]),\n",
      "indices=tensor([1, 0]))\n",
      "torch.return_types.mode(\n",
      "values=tensor([2, 2, 3]),\n",
      "indices=tensor([0, 0, 0]))\n"
     ]
    }
   ],
   "source": [
    "# a example for torch.mode\n",
    "a = torch.tensor([[2, 2, 3], [4, 5, 6]])\n",
    "print(torch.mode(a, dim=1))\n",
    "print(torch.mode(a, dim=0))\n",
    "print(torch.mode(a, dim=-1))\n",
    "print(torch.mode(a, dim=-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
