{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zcwang/.conda/envs/trace/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/zcwang/.conda/envs/trace/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "model_name_or_path=\"/home/zcwang/data/model/Qwen/Qwen1.5-1.8B-Chat\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Please make it concise, informative, and engaging for potential readers.\n",
      "Title: Exploring the Power of Exponential Moving Average Attention for Long-Text Processing and Generation\n",
      "\n",
      "Abstract:\n",
      "\n",
      "NeurIPS invites you to explore the transformative impact of introducing an exponentially moving average (EMA) layer into a transformer-based model architecture for long-text processing and generation. Our proposed ExeMA architecture significantly outperforms vanilla transformers by enhancing long-range understanding while maintaining high fluency.\n",
      "\n",
      "Explain the key contributions of our approach:\n",
      "\n",
      "1. **Attention Heads Replacement**: In traditional transformer architectures, attention heads compute global contextual information for each token, leading to strong correlations across different tokens. This results in suboptimal local performance for tasks like sentence ranking and text generation. By replacing part of the attention heads with EMA, we introduce a novel structure that learns decay rates dynamically for each head, enabling the model to capture diverse patterns in input sequences and effectively transfer knowledge across multiple context words.\n",
      "\n",
      "2. **Perplexity Difference**: Despite being highly competitive in language modeling tasks, the perplexity gap between our ExeMA model and vanilla transformers on SCROLLS benchmark remains relatively small. This suggests that our model excels at capturing longer-range dependencies in long-form text, as evidenced by the significant improvement in coherence observed compared to the baseline. CoGnaTe, a novel coherence metric specifically tailored for long-text processing, further highlights the benefits of our ExeMA approach.\n",
      "\n",
      "3. **Eco-friendly Performance**: The low perplexity and improved coherence achieved by our ExeMA model contribute to its eco-friendly nature. Transformer models often require significant computational resources, particularly when training large datasets. By adopting EMA, we reduce the memory footprint and inference time of the model, making it well-suited for resource-constrained applications such as real-time chatbots or mobile apps.\n",
      "\n",
      "The Impact on Long-Text Processing and Generation:\n",
      "\n",
      "Our ExeMA architecture has numerous practical implications for long-text processing and generation. For instance, it enables the following advancements:\n",
      "\n",
      "- Improved Language Modeling: Our model demonstrates state-of-the-art performance in language modeling tasks, demonstrating that the addition of EMA can significantly boost long-term understanding.\n",
      "- Enhanced Text Generation: With CoGnaTe, our model generates text that is 43% more coherent than the baseline, providing insights into how long-range relationships are encoded in language data.\n",
      "- Scalability: The use of EMA reduces the computational burden associated with transformers, allowing models to handle larger volumes of long-form text without compromising performance.\n",
      "- Reduced Memory Footprint:\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "prompt = \"I'm writing a NeurIPS paper about a new model architecture for processing and generating long texts. Here are some facts about the paper:\\n* The main trick is to replace some of the attention heads with an exponential moving average, where the decay rate is learned for each head. We call this architecture ExeMA.\\n* On language modeling, the perplexity difference between our model and a vanilla transformer is negligible, but that's because next-token prediction is almost always a local task, so perplexity won't be sensitive enough to detect any improvements in long-range understanding.\\n* However, on the SCROLLS benchmark, our model improves by 10% over the baseline.\\n* We also have a new metric for measuring coherence in generated text (CoGnaTe), where our model generates text that is 43% more coherent than the baseline.\\nHelp me write the paper's introduction.\"\n",
    "# messages = [\n",
    "#     {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "#     {\"role\": \"user\", \"content\": prompt}\n",
    "# ]\n",
    "# text = tokenizer.apply_chat_template(\n",
    "#     messages,\n",
    "#     tokenize=False,\n",
    "#     add_generation_prompt=True\n",
    "# )\n",
    "# model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "model_inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    model_inputs.input_ids,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
